
!sudo apt-get update && sudo apt-get install -y ffmpeg


!pip install streamlit pyngrok openai yt-dlp chromadb \
             torch transformers accelerate librosa opencv-python-headless moviepy
# ---------------------------------------------------------
%%writefile app.py

import streamlit as st
import os
import cv2
import json
import base64
import torch
import chromadb
import numpy as np
from datetime import datetime
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from openai import OpenAI
from moviepy.editor import VideoFileClip # â˜… ì•ˆì „í•œ ë³€í™˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ---------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • & í‚¤ ë¡œë“œ (Streamlit Secrets ì‚¬ìš©)
# ---------------------------------------------------------
st.set_page_config(page_title="Think:it", layout="wide")

try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except:
    st.error("ğŸš¨ [ì˜¤ë¥˜] í‚¤ê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ì‹¤í–‰ ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# ë°ì´í„° ê²½ë¡œ
BASE_DIR = "/content/thinkit_data"
os.makedirs(BASE_DIR, exist_ok=True)

# ëª¨ë¸ ë¡œë“œ (ìºì‹±)
@st.cache_resource
def load_whisper_model():
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    
    model_id = "openai/whisper-large-v3"
    
    try:
        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
        )
        model.to(device)
        processor = AutoProcessor.from_pretrained(model_id)
        
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            max_new_tokens=128,
            chunk_length_s=30,
            batch_size=16,
            return_timestamps=True,
            torch_dtype=torch_dtype,
            device=device,
        )
        return pipe
    except Exception as e:
        st.error(f"Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

pipe = load_whisper_model()

# ---------------------------------------------------------
# 2. ë¶„ì„ í•¨ìˆ˜ (Logic)
# ---------------------------------------------------------
def extract_data(video_path):
    if pipe is None: return "", 0, []

    # [ìˆ˜ì •] A. ì˜¤ë””ì˜¤ ì¶”ì¶œ (MoviePy ì‚¬ìš© - í›¨ì”¬ ì•ˆì •ì ì„)
    audio_path = "temp_audio.mp3"
    
    try:
        # ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ
        video_clip = VideoFileClip(video_path)
        
        # ì˜¤ë””ì˜¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        if video_clip.audio is None:
            return "", 0, [] # ì˜¤ë””ì˜¤ ì—†ìŒ
            
        # ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥
        video_clip.audio.write_audiofile(audio_path, logger=None) # ë¡œê·¸ ìˆ¨ê¹€
        video_clip.close()
        
        # B. Whisper ë¶„ì„ (ìƒì„±ëœ mp3 íŒŒì¼ ì½ê¸°)
        audio_result = pipe(audio_path, generate_kwargs={"language": "korean"})
        full_text = audio_result["text"]
        duration = audio_result.get("chunks", [])[-1]['timestamp'][1] if audio_result.get("chunks") else 1
        wpm = (len(full_text.split()) / duration * 60)
        
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return "", 0, []
        
    finally:
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
        if os.path.exists(audio_path):
            os.remove(audio_path)
    
    # C. Visual Analysis
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    frames = []
    if total_frames > 30: points = [0.2, 0.5, 0.8]
    else: points = [0.5]
        
    for point in points:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(total_frames * point))
        success, frame = cap.read()
        if success:
            frame = cv2.resize(frame, (640, 360))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
    cap.release()
    
    return full_text, wpm, frames

# ---------------------------------------------------------
# 3. UI êµ¬ì„±
# ---------------------------------------------------------
st.title("ğŸ¬ Think:it | AI ìœ íŠœë¸Œ ì»¨ì„¤í„´íŠ¸")
st.markdown("ë‹¹ì‹ ì˜ ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”. **ì„±ê³µ ê³µì‹**ê³¼ ë¹„êµí•˜ì—¬ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    category = st.selectbox("ì¹´í…Œê³ ë¦¬ ì„ íƒ", ["Vlog (ì¼ìƒ)", "Entertainment (ì˜ˆëŠ¥)", "Cooking (ìš”ë¦¬)", "Tech (IT)"])
    st.info(f"ğŸ’¡ í˜„ì¬ '{category}' ì¹´í…Œê³ ë¦¬ì˜ ì„±ê³µ ê³µì‹ì„ ì ìš© ì¤‘ì…ë‹ˆë‹¤.")
    
    st.markdown("---")
    st.caption("ì˜¤ëŠ˜ì˜ ì„±ê³µ ê³µì‹ (Updated 09:00)")
    st.markdown(f"""
    - **WPM**: 160 ~ 190
    - **í‚¤ì›Œë“œ**: 'ì¶©ê²©', 'ê²°ë§'
    - **ìŠ¤íƒ€ì¼**: ê³ ì±„ë„, ì–¼êµ´ í´ë¡œì¦ˆì—…
    """)

uploaded_file = st.file_uploader("ë¶„ì„í•  ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ (MP4)", type=["mp4"])

if uploaded_file is not None:
    # ì•ˆì „í•œ íŒŒì¼ëª… ì‚¬ìš©
    temp_path = "input_video.mp4"
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    st.video(temp_path)
    
    if st.button("ğŸš€ AI ë¶„ì„ ì‹œì‘í•˜ê¸°", type="primary"):
        with st.spinner('AIê°€ ì˜ìƒì„ ë³´ê³ , ë“£ê³ , ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1ë¶„ ì†Œìš”)'):
            try:
                # 1. ë°ì´í„° ì¶”ì¶œ
                text, wpm, frames = extract_data(temp_path)
                
                if not text:
                    st.warning("âš ï¸ ì˜ìƒì—ì„œ ëª©ì†Œë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í˜¹ì€ ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ì—†ëŠ” ì˜ìƒì…ë‹ˆë‹¤)")
                else:
                    # 2. GPT-4o ì»¨ì„¤íŒ…
                    prompt = f"""
                    You are a YouTube Consultant.
                    Category: {category}
                    User Data -> WPM: {wpm:.1f}, Script: {text[:300]}...
                    
                    Provide a JSON response in Korean:
                    {{
                        "score": (int 0-100),
                        "comment": (one line summary),
                        "title_ideas": ["title1", "title2", "title3"],
                        "feedback": {{"audio": "...", "visual": "..."}}
                    }}
                    """
                    
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{"role": "user", "content": prompt}],
                        response_format={"type": "json_object"}
                    )
                    result = json.loads(response.choices[0].message.content)
                    
                    # 4. ê²°ê³¼ ë¦¬í¬íŠ¸ í™”ë©´
                    st.divider()
                    st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸")
                    
                    col1, col2, col3 = st.columns(3)
                    col1.metric("íŠ¸ë Œë“œ ì í•©ë„", f"{result['score']}ì ")
                    col2.metric("ë§í•˜ê¸° ì†ë„ (WPM)", f"{int(wpm)}")
                    col3.info(f"{result['comment']}")
                    
                    st.subheader("ğŸ–¼ï¸ AI ì¶”ì²œ ì¸ë„¤ì¼ ì¥ë©´")
                    if frames:
                        img_cols = st.columns(len(frames))
                        for i, frame in enumerate(frames):
                            with img_cols[i]:
                                st.image(frame, caption=f"ì¶”ì²œ í›„ë³´ {i+1}", use_container_width=True)
                    
                    st.subheader("âœï¸ í´ë¦­ì„ ë¶€ë¥´ëŠ” ì œëª© ì¶”ì²œ")
                    for title in result['title_ideas']:
                        st.success(f"ğŸ“Œ {title}")
                        
                    with st.expander("ğŸ“ ìƒì„¸ í”¼ë“œë°± ë³´ê¸°"):
                        st.write(f"**ğŸ”Š ì˜¤ë””ì˜¤:** {result['feedback']['audio']}")
                        st.write(f"**ğŸ¨ ë¹„ì£¼ì–¼:** {result['feedback']['visual']}")

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
            finally:
                if os.path.exists(temp_path): os.remove(temp_path)

# ---------------------------------------------------------

import time
import os
from pyngrok import ngrok
from google.colab import userdata

print("ğŸ” [ì‹œìŠ¤í…œ] Streamlit ë¹„ë°€ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")

try:
    # 1. ì½”ë© Secretsì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°
    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')
    
    # 2. .streamlit í´ë” ë° secrets.toml íŒŒì¼ ìƒì„± (ìë™)
    os.makedirs(".streamlit", exist_ok=True)
    with open(".streamlit/secrets.toml", "w") as f:
        f.write(f'OPENAI_API_KEY = "{OPENAI_API_KEY}"')
    
    print("âœ… ë¹„ë°€ íŒŒì¼ ìƒì„± ì™„ë£Œ! (í™˜ê²½ ë³€ìˆ˜ ì˜¤ë¥˜ í•´ê²°)")
    
    # 3. Ngrok ì¸ì¦
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)

except Exception as e:
    print(f"\nğŸš¨ [ì˜¤ë¥˜] í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ğŸ‘‰ ì™¼ìª½ ì—´ì‡  ì•„ì´ì½˜ > í‚¤ ì´ë¦„ & 'ë…¸íŠ¸ë¶ ì•¡ì„¸ìŠ¤' ON í™•ì¸ í•„ìˆ˜")
    raise e

# 4. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
ngrok.kill()

# 5. Streamlit ì‹¤í–‰ (ì´ì œ íŒŒì¼ì—ì„œ í‚¤ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤)
get_ipython().system_raw('streamlit run app.py &')

# 6. ëŒ€ê¸°
print("â³ ì„œë²„ ì‹œì‘ ì¤‘... (5ì´ˆ)")
time.sleep(5)

# 7. ì£¼ì†Œ ìƒì„±
try:
    public_url = ngrok.connect(8501)
    print(f"\n========================================================")
    print(f"ğŸ‰ Think:it ì›¹ì‚¬ì´íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ‘‰ ì ‘ì† ì£¼ì†Œ: {public_url.public_url}")
    print(f"========================================================\n")
except Exception as e:
    print(f"âŒ ì—ëŸ¬: {e}")
